## 28. ベイズデータ解析

Gelmanら(2013)はベイズデータ解析の特徴を以下のように述べています。

> ベイズデータ解析と言うときは、観測する量や、知りたい量について、確率モデルを使ってデータから推定を行なう実用的な方法という意味で言っています。

続けて、ベイズ統計が頻度主義の方法とどのように異なるかを記述しています。

> ベイズ法の本質的な特徴は、統計解析に基づく推定の際、不確実性を定量化するのに確率を明示的に使うところにあります。

厳密な頻度主義者は、観測値の相対頻度の極値を確率と見なしますから、パラメータについて確率的に言明することを禁止します。パラメータは固定したもので、確率変数ではないとされるのです。

ベイズ主義者も、パラメータを固定した、未知のものと扱います。しかし頻度主義者とは異なり、パラメータの事前分布と、パラメータの事後分布の両方を使います。これら事前分布・事後分布と事後予測確率は、パラメータと将来の観測値についての知識を特徴づけるためにあります。以下で述べるように、事後分布はベイズ推定の基礎を形づくるものです。

### 28.1 ベイズモデリング

Gelmanら(2013)は、応用ベイズモデリングを以下の3ステップに分解しています。

1. すべての観測可能な量と観測不可能な量とについてのフル確率モデルをつくります。このモデルは、モデリングするデータとそれがどのように集められたかということについて、既存の知識と合致するべきです。

2. 観測された量を条件として未知の量の事後確率を計算します。未知の量には、パラメータのような観測できない量や、将来の観測の予測値のような潜在的には観測可能な量といったものが含まれるでしょう。

3. データに対するモデルの当てはまりを評価します。これには、事後分布の効果を評価することも含まれます。

典型的には、3番目のステップでじゅうぶんな当てはまりが得られるまでこのサイクルは繰り返されます。Stanは、2番目と3番目のステップに含まれる計算を自動化します。

### 28.2 ベイズ推定

#### 基本的な量

ベイズ推定のメカニズムはそのままベイズの定理に従っています。記法を定めて、$y$でデータのような観測された量を表し、$\theta$でパラメータや将来の観測値のような未知の量を表すとします。$y$と$\theta$の両方が確率変数としてモデリングされるでしょう。定数や、ハイパーパラメータ、予測変数のような、既知ですがモデリングされない量を$x$で表すとします。

#### 確率関数

確率関数$p(y,\theta)$は、データ$y$とパラメータ$\theta$の同時確率関数です。定数および予測変数$x$は、明示されていませんが条件に含まれていることになっています。パラメータ$\theta$が与えられたときのデータ$y$の条件付き確率関数$p(y \mid \theta)$はサンプリング確率関数と呼ばれます。また、$y$と$x$を固定して$\theta$の関数と見たときには尤度関数とも呼ばれます。

定数$x$が与えられたときのパラメータについての確率関数$p(\theta)$は事前分布と呼ばれます。というのは、データがまだまったく観測されていないときのパラメータの確率を特徴づけるものだからです。条件付き確率関数$p(\theta \mid y)$は事後分布と呼ばれます。観測データ$y$と定数$x$とが与えられたときのパラメータの確率を特徴づけるものだからです。

#### ベイズの定理

ベイズ推定の技術的な仕組みは、以下の一連の式のように決まっています。これはベイズの定理のさまざまな形式として知られています（ここでも、定数$x$は明示していません）。

![$$ \begin{array}{lll} p(\theta\mid y) &= \frac{p(\theta, y)}{p(y)} & [\mbox{条件付き確率の定義}]\\ &= \frac{p(y\mid\theta)p(\theta)}{p(y)} & [\mbox{連鎖律}] \\ &= \frac{p(y\mid\theta)p(\theta)}{\int_{\Theta}p(y,\theta)d\theta} & [\mbox{全確率の法則}] \\ &= \frac{p(y\mid\theta)p(\theta)}{\int_{\Theta}p(y\mid\theta)p(\theta)d\theta} & [\mbox{連鎖律}] \\ &\propto p(y\mid\theta)p(\theta) & [\mbox{yは固定値}] \end{array} $$](fig/fig01.png)

ベイズの定理は、事後確率$p(\theta \mid y)$を「ひっくり返し」て、尤度$p(y \mid \theta)$と事前分布$p(\theta)$だけからなる項で表します（ここでも、定数および予測変数$x$は明示していません）。最後のステップがStanにとって重要で、必要なのは定係数を除いた確率関数の部分のみです。

#### 予測推定

（与えられたモデルで）データ$y$から推定されるパラメータ$\theta$の推定値の不確実性は、事後分布$p(\theta\mid y)$により特徴づけられます。したがって、事後分布はベイズ予測推定にとってきわめて重要です。

$\tilde{y}$が新しい、おそらくはまだ未知の観測値で、$\tilde{x}$がそれに対応する定数および予測変数とすると、事後予測確率は次式で与えられます。

![$$ p(\tilde{y} \mid y) = \int_{\Theta} p(\tilde{y} \mid \theta)p(\theta\mid y)d\theta $$](fig/fig02.png)

ここでは、もともとの定数および予測変数$x$と、新しい定数および予測変数$\tilde{x}$はともに明示されていません。事後確率自身と同様、予測推定は確率的に特徴づけられます。パラメータ$\theta$の点推定値を使うのではなく、データ$y$（と定数$x$）が与えられたときの$\theta$の事後確率$p(\theta \mid y)$によって重みづけつつ、$\theta$の範囲全体について予測値を平均することにより、予測が行なわれます。

事後分布は、イベントの確率を推定することにも使われます。例えば、パラメータ$\theta_k$が0より大きいという確率は、次式により確率的に特徴づけられます。

![$$ \Pr[\theta_{k}>0] = \int_{\Theta}\mathrm{I}(\theta_{k}>0)p(\theta \mid y)d\theta $$](fig/fig03.png)

指示関数$\mathrm{I}(\phi)$は、命題$\phi$が真ならば1と、そうでなければ0と評価されます。

将来の観測を含む比較も同様にできるでしょう。例えば、$\tilde{y}_n > \tilde{y}_{n'}$となる確率は、以下の事後予測確率関数により特徴づけることができます。

![$$ \Pr[\tilde{y}_{n} > \tilde{y}_{n'}] = \int_{\Theta}\int_{Y}\mathrm{I}(\tilde{y}_{n} > \tilde{y}_{n'})p(\tilde{y}\mid\theta)p(\theta\mid y)d\tilde{y}d\theta $$](fig/fig04.png)

#### 事後予測チェック
パラメータをデータに当てはめた後は、モデルの推定を前向きに走らせることにより、そのパラメータを新しいデータセットをシミュレートするのに使うことができます。すると、このような複製データセットと元のデータとを視覚的あるいは統計的に比較して、モデルの当てはまり具合を調べることができます(Gelman et al., 2013, 6章)。

Stanでは、事後シミュレーションは2通りの方法で生成できます。1つ目の方法は、予測される変数をパラメータとして扱い、`model`ブロックでその分布を定義します。2つ目の方法は、離散変数でも適用可能で、`generated quantitiy`ブロックで乱数発生器を使って複製データを生成します。


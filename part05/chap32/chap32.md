## 32. 変分推定

Stanは34章の変換を利用した自動変分推定アルゴリズムを実装しています。

古典的な変分推定アルゴリズムは，導出が困難でした。我々はまず密度が近似する族を定義し，それから変分最適化問題を解くための族に該当する，モデル特有の量を計算しなければなりませんでした。どちらのステップも，専門的な知識を必要とするものです。その結果，アルゴリズムはモデルと選択される近似のどちらにも関連したものになります。

古典的な変分推定フレームワークを簡単に論じるところから始めましょう。この解説については，Jordan et al.(1990); ainwright and Jordan(2008)，あるいはBishop(2006)のテキストを参照してください。レベルの高い自動微分変分推定(ADVI)についてもフォローしています。詳細はKuckelbir et al.2015を参照してください。

### 32.1 古典的変分推定

変分推定は，事後分布の$p(\theta \mid y)$を単純に近似して，分布$q(\theta \mid \phi)$をパラメータ化します。この近似はカルバック・ライブラー(KL)情報量を最小化することで，真の事後分布に合致します。

$$ \phi^{*} = \mathop{\rm arg~min}\limits_\phi KL [ q(\theta \mid \phi) \parallel p(\theta \mid y)].$$

典型的には，KL情報量は解析的な閉形式の解を持ちません。その代わりに，KL距離の代わりになる，変分下限(ELBO)を最大化します。

$$\mathcal{L}(\phi) = \mathbb{E}_{q(\theta)}[ \log p(y,\theta)] - \mathbb{E}_{q(\theta)}[\log q(\theta \mid \phi)].$$

最初の項は，近似の下での対数同時密度の期待値で，第二の項は変分密度のエントロピーです。ELBOを最大化することは，KL情報量を最小化することになります(Jordan et al.,1999;Bishop, 2006).

### 32.2. 自動変分推定

ADVIはELBOを実座標空間で最大化します。Stanはパラメータを(ポテンシャルで)制約された領域から実座標空間に変換します。ここで結合された変換を$T:\theta \to \zeta$とします。この時$\zeta$は$\mathbb{R}^K$にある変数です。変動する対象(ELBO)は次のようになります。

$$\mathcal{L}(\phi) = \mathbb{E}_{q(\zeta \mid \phi)}\left[ \log p(y,T^{-1}(\zeta)) + \log | \det J_{T^{-1}}(\zeta)|\right] - \mathbb{E}_{q(\zeta \mid \phi)}[\log q(\zeta \mid \phi)].$$

$\zeta$は実座標空間にありますので，変分分散にたいして固定した族を選ぶことができます。我々は完全に因子分解されたガウシアンを選び，

$$ q(\zeta \mid \phi) = Normal(\zeta \mid \mu,\sigma) = \prod_{k=1}^K Normal(\zeta_k \mid \mu_k,\sigma_k)$$

とします。ここでベクトル$\phi = (\mu_1,\dots,\mu_k,\sigma_1,\dots,\sigma_k)$は各ガウシアン因子の平均と標準偏差をつなげたものです。これは古典的変分推定アルゴリズムにおける，「平均場」の仮定を反映しています；この特別な分解については，`meanfield`オプションのところで参照しています。

変換$T$は，実座標空間にパラメータをおく時の台(support)を位置づけます。つまり，$T^{-1}$は潜在変数の台に戻すものになります。これは暗に，元の潜在変数空間における変分近似を，次のように定義することを意味します。

$$ Normal(T(\theta)\mid \mu,\sigma)|\det J_T(\theta)|. $$

これは，一般に，ガウス分布ではありません。この選択は，ラプラス近似テクニックを思い起こさせるかもしれません。そこでは事後分布に対するガウシアン近似に最大事後確率推定をするとき，2階のテイラー展開をするのでした。しかし，これらは同じものではありません(Kucukelbir et al.,2015)。

我々が最大化する変分する対象(ELBO)は，

$$\mathcal{L}(\phi) = \mathbb{E}_{q(\zeta \mid \phi)}\left[ \log p(y,T^{-1}(\zeta)) + \log | \det J_{T^{-1}}(\zeta)|  \right] + \sum_{k=1}^K \log \sigma_k$$

であり，ここではガウシアンエントロピーについて解析的な形で代入し，$\phi$に依存しないすべての項を落としています。この最大化がどのような成果をあげるかについては，36章で論じます。
